{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMM3iWGWiPar"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "folder = r\"C:\\Users\\Srijan Bhushan\\Downloads\\fashion-minst\\final\"\n",
        "\n",
        "def load_data(X, y):\n",
        "    \"\"\"\n",
        "    Loads images and one-hot encoded labels.\n",
        "    [Q1] Why one-hot encoding?\n",
        "    One-hot vectors represent classes as probability distributions needed for cross-entropy\n",
        "    loss and prevent ordinal assumptions about classes.\n",
        "    \"\"\"\n",
        "    \"\"\"Rest all question answers are in seperate pdf file in github link\"\"\"\n",
        "    for class_name in os.listdir(folder):\n",
        "        if not class_name.isdigit():\n",
        "            continue\n",
        "        idx = int(class_name)\n",
        "        class_path = os.path.join(folder, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        files = os.listdir(class_path)\n",
        "        for fn in files:\n",
        "            fp = os.path.join(class_path, fn)\n",
        "            try:\n",
        "                img = Image.open(fp).convert('L')  # grayscale\n",
        "                img = img.resize((28, 28))\n",
        "                X.append(np.array(img))\n",
        "            except Exception as e:\n",
        "                print(f\"Skipped {fp}: {e}\")\n",
        "\n",
        "        label = [0] * 10\n",
        "        label[idx] = 1\n",
        "        y.extend([label] * len(files))\n",
        "        print(f\"Loaded class {class_name}\")\n",
        "\n",
        "X, y = [], []\n",
        "load_data(X, y)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X = X.reshape(X.shape[0], -1)  # Flatten images to 784\n",
        "\n",
        "print(\"Data shapes:\", X.shape, y.shape)\n",
        "\n",
        "class NN:\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, lr, epochs, batch_size):\n",
        "        \"\"\"\n",
        "        Two hidden layers network with batch normalization.\n",
        "        [Q5] Learning rate controls weight update magnitude.\n",
        "        \"\"\"\n",
        "        self.in_n = input_size\n",
        "        self.hid1_n = hidden1_size\n",
        "        self.hid2_n = hidden2_size\n",
        "        self.out_n = output_size\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize weights with He initialization\n",
        "        self.wih1 = np.random.randn(self.hid1_n, self.in_n) * np.sqrt(2 / self.in_n)\n",
        "        self.bih1 = np.zeros((self.hid1_n, 1))\n",
        "        self.wih2 = np.random.randn(self.hid2_n, self.hid1_n) * np.sqrt(2 / self.hid1_n)\n",
        "        self.bih2 = np.zeros((self.hid2_n, 1))\n",
        "        self.who = np.random.randn(self.out_n, self.hid2_n) * np.sqrt(2 / self.hid2_n)\n",
        "        self.bho = np.zeros((self.out_n, 1))\n",
        "\n",
        "    def relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def relu_derivative(self, z):\n",
        "        return (z > 0).astype(float)\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "    def cross_entropy_loss(self, y_true, y_pred):\n",
        "        eps = 1e-15\n",
        "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "        return -np.sum(y_true * np.log(y_pred), axis=0)\n",
        "\n",
        "    def cross_entropy_derivative(self, y_true, y_pred):\n",
        "        return y_pred - y_true\n",
        "\n",
        "    def batch_norm(self, x):\n",
        "        mu = np.mean(x, axis=1, keepdims=True)\n",
        "        var = np.var(x, axis=1, keepdims=True)\n",
        "        x_norm = (x - mu) / (np.sqrt(var + 1e-8))\n",
        "        return x_norm\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = X.T\n",
        "        X = X - np.mean(X, axis=0, keepdims=True)\n",
        "\n",
        "        self.z1 = np.dot(self.wih1, X) + self.bih1\n",
        "        self.a1 = self.batch_norm(self.z1)\n",
        "        self.a1 = self.relu(self.a1)\n",
        "\n",
        "        self.z2 = np.dot(self.wih2, self.a1) + self.bih2\n",
        "        self.a2 = self.batch_norm(self.z2)\n",
        "        self.a2 = self.relu(self.a2)\n",
        "\n",
        "        self.z3 = np.dot(self.who, self.a2) + self.bho\n",
        "        self.a3 = self.softmax(self.z3)\n",
        "        return self.a3\n",
        "\n",
        "    def backprop(self, X, y):\n",
        "        inputs = np.array(X, ndmin=2).T\n",
        "        inputs = inputs - np.mean(inputs, axis=0, keepdims=True)\n",
        "        targets = np.array(y, ndmin=2).T\n",
        "\n",
        "        y_pred = self.forward(X)\n",
        "        m = inputs.shape[1]\n",
        "\n",
        "        dZ3 = self.cross_entropy_derivative(targets, y_pred)\n",
        "        dW3 = np.dot(dZ3, self.a2.T) / m\n",
        "        db3 = np.sum(dZ3, axis=1, keepdims=True) / m\n",
        "\n",
        "        dA2 = np.dot(self.who.T, dZ3)\n",
        "        dZ2 = dA2 * self.relu_derivative(self.a2)  # relu derivative on activated batch-normed output\n",
        "        dW2 = np.dot(dZ2, self.a1.T) / m\n",
        "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
        "\n",
        "        dA1 = np.dot(self.wih2.T, dZ2)\n",
        "        dZ1 = dA1 * self.relu_derivative(self.a1)\n",
        "        dW1 = np.dot(dZ1, inputs.T) / m\n",
        "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
        "\n",
        "        self.who -= self.lr * dW3\n",
        "        self.bho -= self.lr * db3\n",
        "        self.wih2 -= self.lr * dW2\n",
        "        self.bih2 -= self.lr * db2\n",
        "        self.wih1 -= self.lr * dW1\n",
        "        self.bih1 -= self.lr * db1\n",
        "\n",
        "        loss = self.cross_entropy_loss(targets, y_pred)\n",
        "        return np.mean(loss)\n",
        "\n",
        "    def fit(self, X, y, X_val, y_val):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            shuffled_indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[shuffled_indices]\n",
        "            y_shuffled = y[shuffled_indices]\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            for i in range(0, n_samples, self.batch_size):\n",
        "                X_batch = X_shuffled[i:i+self.batch_size]\n",
        "                y_batch = y_shuffled[i:i+self.batch_size]\n",
        "                batch_loss = self.backprop(X_batch, y_batch)\n",
        "                epoch_loss += batch_loss * X_batch.shape[0]\n",
        "\n",
        "            epoch_loss /= n_samples\n",
        "            train_losses.append(epoch_loss)\n",
        "\n",
        "            val_pred = self.forward(X_val).T\n",
        "            val_loss = np.mean(self.cross_entropy_loss(y_val.T, val_pred.T))\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.forward(X).T\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "# Shuffle data and split 70:20:10\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "s1, s2 = int(0.7 * len(X)), int(0.9 * len(X))\n",
        "X_train, y_train = X[:s1], y[:s1]\n",
        "X_val, y_val = X[s1:s2], y[s1:s2]\n",
        "X_test, y_test = X[s2:], y[s2:]\n",
        "\n",
        "# Hyperparameters: deeper network, batch norm, smaller LR, longer epochs\n",
        "model = NN(784, 256, 128, 10, lr=0.005, epochs=300, batch_size=64)\n",
        "\n",
        "train_losses, val_losses = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(f\"Final Test Accuracy: {np.mean(y_pred == y_true):.4f}\")\n"
      ]
    }
  ]
}